{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ninevv/RAG_Project/blob/Zineb/Copie_de_Project_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project : RAG\n",
        "\n",
        "The goal of this project is to create a simple LLMs based RAG module. Obviously the most complex part is how to correctly link all components. We let you free to create your own database. Anything can be ragged, from research paper, songs, subtitles, books,...\n",
        "\n",
        "This project is voluntarly sparsely helped, as, as engineers, you will dig into lots of existing method, and will need to pick the best one up. We want you to get familiar with the engineering world.\n",
        "\n",
        "We will give you some recommendations. You will see lots of issues during this project, some you've already seen during Labs, other that are new. So buckle up, read the docs, and RAG your data.\n",
        "\n",
        "Obviously there are lots of tutorials of the internet that you could just copy paste to get a baseline.\n",
        "\n",
        "\n",
        "## **We encourage you to do code versioning using Github.**\n",
        "\n",
        "\n",
        "**Ideal Project Timeline:**\n",
        "\n",
        "*   Talking and getting to know the Modules. Discussing about the choice of your LLMs and the environment you'll have. Choosing a first set of data to RAG. Setting up your Github (Optional) (1h)\n",
        "*   Setting up your first RAG Chain using Langchain or other (2-3h)\n",
        "*   Understand the limitation of your RAG and find enhancements to set up your 2nd RAG. (2h)\n",
        "*   Unveiling the unknown document, adapt your RAGs (2h)\n",
        "*   Deploy it (Optional)\n",
        "*   Begin your presentation (2h)\n",
        "*   Presentation (5 min/groups)\n",
        "\n",
        "\n",
        "We'll evaluate your presentation quality, your RAG system's capability, and your progress throughout the project sessions.\n",
        "\n",
        "**Students who missed the first session will start with a score of 0 in the progress part**. :-)\n"
      ],
      "metadata": {
        "id": "eLtiBx5FOWer"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Presentation:\n",
        "- The number of slides you can do is unlimited\n",
        "- You only have 5 min to present your project. We will stop you at 5 min whether you've finished or not\n",
        "- Your presentation should include:\n",
        "  - a presentation of your workflow (Agile Methodology, What's the job of each one...)\n",
        "  - a presentation of your final pipeline with all enhancements done\n",
        "  - a proof a work of your RAG on your data, and on the unknown data\n",
        "  - what limitations you have and how to tackle them. For each too obvious limitations (more GPUs, more RAM..) : -1\n",
        "  - if you've deployed your RAG, a scannable QR code to live test it."
      ],
      "metadata": {
        "id": "Jd-X5ehunElN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Preliminaries : Some useful downloads\n",
        "\n",
        "We give you some useful frameworks, that you could use to build your RAG."
      ],
      "metadata": {
        "id": "53l6aIRMhsJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pypdf python-dotenv\n",
        "!pip install transformers\n",
        "!pip install -q datasets loralib sentencepiece\n",
        "!pip install -q einops accelerate langchain bitsandbytes\n",
        "!pip install sentence_transformers\n",
        "!pip install llama-index\n",
        "!%pip install --upgrade --quiet  langchain langchain-openai faiss-cpu tiktoken\n"
      ],
      "metadata": {
        "id": "pty32z_YuW_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "3UimK2nYjfrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai"
      ],
      "metadata": {
        "id": "BOg9xicsjT9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
        "\n",
        "# import dotenv\n",
        "\n",
        "# dotenv.load_dotenv()"
      ],
      "metadata": {
        "id": "t0m9uGeChuAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I - Data Sourcing\n",
        "\n",
        "Data Sourcing for RAG is really simple. Some questions to guide you:\n",
        "\n",
        "* What data do we need ?\n",
        "* How can we correctly parse the data ?\n",
        "* Does the vector index provides a good representation for the vector database ?\n",
        "* For example, using FAISS, can you easily retrieve your document ?\n",
        "\n",
        "To begin, pick a simple story and store it in a Vector Database. You have the choice between multiple VectorStores (ChromaDB, QDrant,...)\n"
      ],
      "metadata": {
        "id": "NmBmFi1LzltU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Indexing : Loading data #"
      ],
      "metadata": {
        "id": "CVTUyFA3DB4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PDF#"
      ],
      "metadata": {
        "id": "YQ0a1SIQDJoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf\n",
        "!pip install rapidocr-onnxruntime"
      ],
      "metadata": {
        "id": "W7tJxuKbDBic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain-text-splitters\n",
        "from langchain_community.document_loaders import WebBaseLoader"
      ],
      "metadata": {
        "id": "ZgmW6mq_-wze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"RAG_Data/Definition of literary movement.pdf\", extract_images=True)\n",
        "pages = loader.load_and_split()"
      ],
      "metadata": {
        "id": "xkQ6IbZLEGk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages[0]"
      ],
      "metadata": {
        "id": "XS76Foh3EVc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Indexing : Split#"
      ],
      "metadata": {
        "id": "BCmxoUZNBC9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    add_start_index=True\n",
        ")\n",
        "all_splits = text_splitter.split_documents(pages)"
      ],
      "metadata": {
        "id": "9yexkB_UBCB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_splits[0]\n",
        "len(all_splits)"
      ],
      "metadata": {
        "id": "E7ersL_DMppR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_splits[0].page_content\n",
        "len(all_splits[0].page_content)"
      ],
      "metadata": {
        "id": "P90S9sy0Mun-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_splits[0].metadata"
      ],
      "metadata": {
        "id": "uGaVK9-EMowP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Indexing: Store#\n"
      ],
      "metadata": {
        "id": "_fZaYNXwffy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings(),\n",
        "                                    persist_directory=\"./chroma_db\")"
      ],
      "metadata": {
        "id": "3h97Tzzrespb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Retrieval and Generation#"
      ],
      "metadata": {
        "id": "L9R0yYrKkI61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})"
      ],
      "metadata": {
        "id": "sXObdMP7kLu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = r\" What is a literary movement ?\"\n",
        "retrieved_docs = retriever.invoke(query)"
      ],
      "metadata": {
        "id": "npNoDXlckfe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(retrieved_docs)"
      ],
      "metadata": {
        "id": "9OQyY1lBk3SY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(retrieved_docs[0].page_content)"
      ],
      "metadata": {
        "id": "t3H86kb-k9yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save to disk\n",
        "# db2 = Chroma.from_documents(docs, embedding_function, persist_directory=\"./chroma_db\")\n",
        "vectorstore.persist()\n",
        "docs = vectorstore.similarity_search(query)"
      ],
      "metadata": {
        "id": "P-VV96B49vcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "id": "zhtz2R_v-av1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II - Module Creation\n",
        "\n",
        "Should you create a RAG Module, you need or not a LLM. We encourage you to test some LLMs (Mistral, LLama, Falcon, Gemma, ...) However, be aware that you won't have the space to run it on this colab.\n",
        "\n",
        "We highly recommend to use LangChain, to build your Q&A app.\n",
        "\n",
        "Some Questions to guide you:\n",
        "* What model is easily accesible\n",
        "* Are there any existing code to begin with ?\n",
        "* What about the prompts ?\n",
        "* What about the document parsing ?\n"
      ],
      "metadata": {
        "id": "JPNynNQm0z4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# III - Model Serving (Optional and for the best)\n",
        "\n",
        "You can inspire yourself from the first hands-on, if your feeling powerful, you can build something using fastapi and push your deployed RAG into a simple github.io instance. Or just use the gradio deploiement framework."
      ],
      "metadata": {
        "id": "JeMHYQwc02uG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##"
      ],
      "metadata": {
        "id": "SjiQrHTgdWGm"
      }
    }
  ]
}
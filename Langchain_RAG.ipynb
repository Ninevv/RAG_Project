{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "dPgiiJnEOhhf",
        "8ZIUq5MoOpH2",
        "djh8VsZ3Otbs",
        "5lZaA8psqWMJ",
        "q24lswpQMP6N"
      ],
      "authorship_tag": "ABX9TyNcqhShqPz+i1N8BxZ0VgSu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ninevv/RAG_Project/blob/main/Langchain_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dependencies#"
      ],
      "metadata": {
        "id": "dPgiiJnEOhhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pypdf python-dotenv\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install -q datasets loralib sentencepiece\n",
        "!pip install -q einops accelerate langchain bitsandbytes\n",
        "!pip install sentence_transformers\n",
        "!pip install llama-index\n",
        "!%pip install --upgrade --quiet  langchain langchain-openai faiss-cpu tiktoken"
      ],
      "metadata": {
        "id": "cKa4uQmwBvLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai chromadb bs4"
      ],
      "metadata": {
        "id": "rD0IHN0ltlZ_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "id": "mv-6yJ2tGVjz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d261d654-f133-4b7a-ce2c-37d43644e9ec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#First tests#"
      ],
      "metadata": {
        "id": "8ZIUq5MoOpH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_of_documents = [\n",
        "    \"Take a leisurely walk in the park and enjoy the fresh air.\",\n",
        "    \"Visit a local museum and discover something new.\",\n",
        "    \"Attend a live music concert and feel the rhythm.\",\n",
        "    \"Go for a hike and admire the natural scenery.\",\n",
        "    \"Have a picnic with friends and share some laughs.\",\n",
        "    \"Explore a new cuisine by dining at an ethnic restaurant.\",\n",
        "    \"Take a yoga class and stretch your body and mind.\",\n",
        "    \"Join a local sports league and enjoy some friendly competition.\",\n",
        "    \"Attend a workshop or lecture on a topic you're interested in.\",\n",
        "    \"Visit an amusement park and ride the roller coasters.\"\n",
        "]"
      ],
      "metadata": {
        "id": "0axsnLVyFoLl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_similarity(query, document):\n",
        "    query = query.lower().split(\" \")\n",
        "    document = document.lower().split(\" \")\n",
        "    intersection = set(query).intersection(set(document))\n",
        "    union = set(query).union(set(document))\n",
        "    return len(intersection)/len(union)"
      ],
      "metadata": {
        "id": "7oKGFPC7Ga9W"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def return_response(query, corpus):\n",
        "    similarities = []\n",
        "    for doc in corpus:\n",
        "        similarity = jaccard_similarity(user_input, doc)\n",
        "        similarities.append(similarity)\n",
        "    return corpus_of_documents[similarities.index(max(similarities))]"
      ],
      "metadata": {
        "id": "yXqWAj2CG3KJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = \"What is a leisure activity that you like?\""
      ],
      "metadata": {
        "id": "3_cZYC2NGfRc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"I like to hike\""
      ],
      "metadata": {
        "id": "NDHozCTvGh2R"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "return_response(user_input, corpus_of_documents)"
      ],
      "metadata": {
        "id": "PwpWHRmNGyko",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "13ed673d-f399-42c9-8eba-a77b3fb1df14"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Go for a hike and admire the natural scenery.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#More dependencies#"
      ],
      "metadata": {
        "id": "djh8VsZ3Otbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "ZjqeqZc-Tkhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"HF_TOKEN\""
      ],
      "metadata": {
        "id": "nUFTYSQfTmNz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import HuggingFaceEndpoint"
      ],
      "metadata": {
        "id": "OKH4_rrZTqBT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "o5LWbszRyWPQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you want more control, you will need to define the tokenizer and model.\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer"
      ],
      "metadata": {
        "id": "6c6HANGaMf3k"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes==0.43.0\n",
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "f1qx6617JvZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specify how to quantize the model\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", quantization_config=quantization_config, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "\n",
        "# With pipeline, just specify the task and the model id from the Hub.\n",
        "from transformers import pipeline\n",
        "pipe = pipeline(\"text-generation\", model=model,tokenizer=tokenizer, max_new_tokens=10)"
      ],
      "metadata": {
        "id": "SKvTYq69AhLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(\"What is the name of this restaurant?\")"
      ],
      "metadata": {
        "id": "7Y7CiOdBnBss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Indexing : Load#"
      ],
      "metadata": {
        "id": "iFWL4onlnckm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "# Only keep post title, headers, and content from the full HTML.\n",
        "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
        ")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "diW2C9NeMKKD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"RAG_Data/Definition of literary movement.pdf\", extract_images=True)\n",
        "pages = loader.load_and_split()"
      ],
      "metadata": {
        "id": "n4D83buXQgzQ"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(pages[0].page_content)"
      ],
      "metadata": {
        "id": "WTdaS43RHgR3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a40282a3-a8a1-41d0-c0d7-3dcfe835ed2f"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2401"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pages[0].page_content[:500])"
      ],
      "metadata": {
        "id": "aPS6Ga49MNgb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51738ebd-563c-4651-aa6a-83872e3b39e6"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Definition of literary movement  \n",
            "  \n",
            "A literary movement is a term that brings together authors whose writing style and vision of \n",
            "literature share many common traits. Sometimes, these authors form veritable schools (such as \n",
            "naturalism) with a common project.  \n",
            "  \n",
            "   \n",
            "  \n",
            "Humanism  \n",
            "  \n",
            "Humanism, a 19th -century term, refers to a European literary and philosophical cultural movement \n",
            "of the 15th and 16th centuries, corresponding to the Renaissance.  \n",
            "  \n",
            "In France, the main figures of humanism as \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Indexing : Split#"
      ],
      "metadata": {
        "id": "sn3iEcinnlcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
        ")\n",
        "all_splits = text_splitter.split_documents(pages)"
      ],
      "metadata": {
        "id": "YPcCqox5MQi9"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_splits)"
      ],
      "metadata": {
        "id": "K8kH-f-XMVb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcc81113-0d78-4fe2-dc23-0883a131475e"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "61"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_splits[0].page_content)"
      ],
      "metadata": {
        "id": "wZ4bm9xzMXxZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b634d700-7f9c-4cbb-fbf1-522a22a04670"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "954"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_splits[10].metadata"
      ],
      "metadata": {
        "id": "B-KbkXr8MZxB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa2ea772-20a7-470c-9b6e-2b80859f3731"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'source': 'RAG_Data/Definition of literary movement.pdf',\n",
              " 'page': 3,\n",
              " 'start_index': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Context aware splitter rag q&a quickstart#"
      ],
      "metadata": {
        "id": "spNC__gmMgQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Indexing : Store#"
      ],
      "metadata": {
        "id": "BH023H3VMp9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma #vs FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# create the open-source embedding function\n",
        "embedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# load it into Chroma\n",
        "vectorstore = Chroma.from_documents(documents=all_splits, embedding=embedding_function, persist_directory=\"./chroma_db\")"
      ],
      "metadata": {
        "id": "aE5_jAP7NsMw"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Retrieval and Generation : Retrieve #"
      ],
      "metadata": {
        "id": "Wua0AmMJ4Fv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
        "retrieved_docs = retriever.invoke(\"What are the approaches to Task Decomposition?\")\n",
        "len(retrieved_docs)"
      ],
      "metadata": {
        "id": "B-QtxHVeQ9vc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66be0302-fad0-44c7-d8cb-1069ffa7bf76"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(retrieved_docs[0].page_content)"
      ],
      "metadata": {
        "id": "gAq8T-KHREpp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9559383a-fe55-4743-cda2-9f00f99f5bb9"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
            "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# query it\n",
        "query = \"What did the president say about Ketanji Brown Jackson\"\n",
        "docs = vectorstore.similarity_search(query)\n",
        "\n",
        "# print results\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "HRYHug8WTn5g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6acd9af-baa0-482a-b768-545d6517027a"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You should only respond in JSON format as described below\n",
            "Response Format:\n",
            "{\n",
            "    \"thoughts\": {\n",
            "        \"text\": \"thought\",\n",
            "        \"reasoning\": \"reasoning\",\n",
            "        \"plan\": \"- short bulleted\\n- list that conveys\\n- long-term plan\",\n",
            "        \"criticism\": \"constructive self-criticism\",\n",
            "        \"speak\": \"thoughts summary to say to user\"\n",
            "    },\n",
            "    \"command\": {\n",
            "        \"name\": \"command name\",\n",
            "        \"args\": {\n",
            "            \"arg name\": \"value\"\n",
            "        }\n",
            "    }\n",
            "}\n",
            "Ensure the response can be parsed by Python json.loads\n",
            "GPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\n",
            "Here are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\n",
            "[\n",
            "  {\n",
            "    \"role\": \"system\",\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Retrieval and Generation : Generate #"
      ],
      "metadata": {
        "id": "caUOuJgbSDI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "_u8NZXPdRcRs"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ],
      "metadata": {
        "id": "rpI-DROHRmUe"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "model = ChatOpenAI()\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "setup_and_retrieval = RunnableParallel(\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        ")\n",
        "chain = setup_and_retrieval | prompt | model | output_parser\n",
        "\n",
        "chain.invoke(\"what is a literary movement?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "3X64sewWND3R",
        "outputId": "f1a27112-00b7-4aa9-8dca-498389e28245"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A literary movement is a term that brings together authors whose writing style and vision of literature share many common traits. It can sometimes involve authors forming schools with a common project.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optional : costumize prompt#"
      ],
      "metadata": {
        "id": "5lZaA8psqWMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#costumize prompt\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Use three sentences maximum and keep the answer as concise as possible.\n",
        "Always say \"thanks for asking!\" at the end of the answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Helpful Answer:\"\"\"\n",
        "custom_rag_prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | custom_rag_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "rag_chain.invoke(\"What is Task Decomposition?\")"
      ],
      "metadata": {
        "id": "Pvw8tEGvR5pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optional#"
      ],
      "metadata": {
        "id": "q24lswpQMP6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save to disk\n",
        "db2 = Chroma.from_documents(docs, embedding_function, persist_directory=\"./chroma_db\")\n",
        "docs = db2.similarity_search(query)\n",
        "\n",
        "# load from disk\n",
        "db3 = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_function)\n",
        "docs = db3.similarity_search(query)\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "aFIfQLDfOx3a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
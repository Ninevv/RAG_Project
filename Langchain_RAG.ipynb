{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPdGun2RAnRj82wmawfpGZq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ninevv/RAG_Project/blob/Zineb/Langchain_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pypdf python-dotenv\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install -q datasets loralib sentencepiece\n",
        "!pip install -q einops accelerate langchain bitsandbytes\n",
        "!pip install sentence_transformers\n",
        "!pip install llama-index\n",
        "!%pip install --upgrade --quiet  langchain langchain-openai faiss-cpu tiktoken"
      ],
      "metadata": {
        "id": "cKa4uQmwBvLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai chromadb bs4"
      ],
      "metadata": {
        "id": "rD0IHN0ltlZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "mv-6yJ2tGVjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_of_documents = [\n",
        "    \"Take a leisurely walk in the park and enjoy the fresh air.\",\n",
        "    \"Visit a local museum and discover something new.\",\n",
        "    \"Attend a live music concert and feel the rhythm.\",\n",
        "    \"Go for a hike and admire the natural scenery.\",\n",
        "    \"Have a picnic with friends and share some laughs.\",\n",
        "    \"Explore a new cuisine by dining at an ethnic restaurant.\",\n",
        "    \"Take a yoga class and stretch your body and mind.\",\n",
        "    \"Join a local sports league and enjoy some friendly competition.\",\n",
        "    \"Attend a workshop or lecture on a topic you're interested in.\",\n",
        "    \"Visit an amusement park and ride the roller coasters.\"\n",
        "]"
      ],
      "metadata": {
        "id": "0axsnLVyFoLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_similarity(query, document):\n",
        "    query = query.lower().split(\" \")\n",
        "    document = document.lower().split(\" \")\n",
        "    intersection = set(query).intersection(set(document))\n",
        "    union = set(query).union(set(document))\n",
        "    return len(intersection)/len(union)"
      ],
      "metadata": {
        "id": "7oKGFPC7Ga9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def return_response(query, corpus):\n",
        "    similarities = []\n",
        "    for doc in corpus:\n",
        "        similarity = jaccard_similarity(user_input, doc)\n",
        "        similarities.append(similarity)\n",
        "    return corpus_of_documents[similarities.index(max(similarities))]"
      ],
      "metadata": {
        "id": "yXqWAj2CG3KJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = \"What is a leisure activity that you like?\""
      ],
      "metadata": {
        "id": "3_cZYC2NGfRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"I like to hike\""
      ],
      "metadata": {
        "id": "NDHozCTvGh2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "return_response(user_input, corpus_of_documents)"
      ],
      "metadata": {
        "id": "PwpWHRmNGyko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "ZjqeqZc-Tkhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"HF_TOKEN\""
      ],
      "metadata": {
        "id": "nUFTYSQfTmNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import HuggingFaceEndpoint"
      ],
      "metadata": {
        "id": "OKH4_rrZTqBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "o5LWbszRyWPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you want more control, you will need to define the tokenizer and model.\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer"
      ],
      "metadata": {
        "id": "6c6HANGaMf3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bitsandbytes\n",
        "import accelerate"
      ],
      "metadata": {
        "id": "f1qx6617JvZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specify how to quantize the model\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", quantization_config=quantization_config, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "\n",
        "# With pipeline, just specify the task and the model id from the Hub.\n",
        "from transformers import pipeline\n",
        "pipe = pipeline(\"text-generation\", model=model,tokenizer=tokenizer, max_new_tokens=10)"
      ],
      "metadata": {
        "id": "SKvTYq69AhLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(\"What is the name of this restaurant?\")"
      ],
      "metadata": {
        "id": "7Y7CiOdBnBss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Indexing : Load#"
      ],
      "metadata": {
        "id": "iFWL4onlnckm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "# Only keep post title, headers, and content from the full HTML.\n",
        "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
        ")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "diW2C9NeMKKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs[0].page_content)"
      ],
      "metadata": {
        "id": "WTdaS43RHgR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].page_content[:500])"
      ],
      "metadata": {
        "id": "aPS6Ga49MNgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Indexing : Split#"
      ],
      "metadata": {
        "id": "sn3iEcinnlcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
        ")\n",
        "all_splits = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "YPcCqox5MQi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_splits)"
      ],
      "metadata": {
        "id": "K8kH-f-XMVb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_splits[0].page_content)"
      ],
      "metadata": {
        "id": "wZ4bm9xzMXxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_splits[10].metadata"
      ],
      "metadata": {
        "id": "B-KbkXr8MZxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Context aware splitter rag q&a quickstart#"
      ],
      "metadata": {
        "id": "spNC__gmMgQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Indexing : Store#"
      ],
      "metadata": {
        "id": "BH023H3VMp9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma #vs FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# create the open-source embedding function\n",
        "embedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# load it into Chroma\n",
        "vectorstore = Chroma.from_documents(documents=all_splits, embedding=embedding_function, persist_directory=\"./chroma_db\")"
      ],
      "metadata": {
        "id": "aE5_jAP7NsMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
        "retrieved_docs = retriever.invoke(\"What are the approaches to Task Decomposition?\")\n",
        "len(retrieved_docs)"
      ],
      "metadata": {
        "id": "B-QtxHVeQ9vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(retrieved_docs[0].page_content)"
      ],
      "metadata": {
        "id": "gAq8T-KHREpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# query it\n",
        "query = \"What did the president say about Ketanji Brown Jackson\"\n",
        "docs = vectorstore.similarity_search(query)\n",
        "\n",
        "# print results\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "HRYHug8WTn5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Retrieval and Generation#"
      ],
      "metadata": {
        "id": "caUOuJgbSDI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = model"
      ],
      "metadata": {
        "id": "11v0H1RIRVVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")"
      ],
      "metadata": {
        "id": "_u8NZXPdRcRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_messages = prompt.invoke(\n",
        "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
        ").to_messages()\n",
        "example_messages"
      ],
      "metadata": {
        "id": "MvsqFX5ORh62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(example_messages[0].content)"
      ],
      "metadata": {
        "id": "Q9D1CyN5RkbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "rpI-DROHRmUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain.stream"
      ],
      "metadata": {
        "id": "iUZX-7Z7yAOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in rag_chain.stream(\"What is Task Decomposition\"):\n",
        "    print(chunk, end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "HNAFRZsFRp5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optional : costumize prompt#"
      ],
      "metadata": {
        "id": "5lZaA8psqWMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#costumize prompt\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Use three sentences maximum and keep the answer as concise as possible.\n",
        "Always say \"thanks for asking!\" at the end of the answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Helpful Answer:\"\"\"\n",
        "custom_rag_prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | custom_rag_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "rag_chain.invoke(\"What is Task Decomposition?\")"
      ],
      "metadata": {
        "id": "Pvw8tEGvR5pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save to disk\n",
        "db2 = Chroma.from_documents(docs, embedding_function, persist_directory=\"./chroma_db\")\n",
        "docs = db2.similarity_search(query)\n",
        "\n",
        "# load from disk\n",
        "db3 = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_function)\n",
        "docs = db3.similarity_search(query)\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "aFIfQLDfOx3a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}